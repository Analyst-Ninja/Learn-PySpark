{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bbbf42-b624-4c37-8cde-b1447744661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/15 15:13:54 WARN Utils: Your hostname, codebase resolves to a loopback address: 127.0.1.1; using 172.27.188.31 instead (on interface eth0)\n",
      "24/06/15 15:13:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/15 15:13:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.Builder().appName('ReadCSVtoDataFrame').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "408be54a-8c59-4455-9a6c-3d74f9f3c54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|  1|  Rohit|  1000|\n",
      "|  2|   Ajay|  2000|\n",
      "|  3|Nirmala|  3000|\n",
      "|  4|    Ram|  4000|\n",
      "+---+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read.option('header', True).csv(path='data/Employee1.csv')\n",
    "\n",
    "# OR\n",
    "\n",
    "df = spark.read.csv(path='data/Employee1.csv', header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e17976e0-8cde-48ff-a34c-8ede8a0dce29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|  1|  Rohit|  1000|\n",
      "|  2|   Ajay|  2000|\n",
      "|  3|Nirmala|  3000|\n",
      "|  4|    Ram|  4000|\n",
      "+---+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option(key='header', value=True).load(path='data/Employee1.csv')\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783ac4c-3e73-4564-8a92-fc0ed4d35245",
   "metadata": {},
   "source": [
    "#### Reading a list of CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01958360-4ae8-4621-b0ac-4928cf3dd44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|  1|  Rohit|  1000|\n",
      "|  2|   Ajay|  2000|\n",
      "|  3|Nirmala|  3000|\n",
      "|  4|    Ram|  4000|\n",
      "|  5|  Rohit|  1000|\n",
      "|  6|   Ajay|  2000|\n",
      "|  7|Nirmala|  3000|\n",
      "|  8|    Ram|  4000|\n",
      "+---+-------+------+\n",
      "\n",
      "--> Another Way --> If we want to give directory / folder loaction only <--\n",
      "\n",
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|  1|  Rohit|  1000|\n",
      "|  2|   Ajay|  2000|\n",
      "|  3|Nirmala|  3000|\n",
      "|  4|    Ram|  4000|\n",
      "|  5|  Rohit|  1000|\n",
      "|  6|   Ajay|  2000|\n",
      "|  7|Nirmala|  3000|\n",
      "|  8|    Ram|  4000|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', True).csv(path=['data/Employee1.csv','data/Employee2.csv']).show()\n",
    "\n",
    "# OR --> Another Way --> If we want to give directory / folder loaction only\n",
    "print(\"--> Another Way --> If we want to give directory / folder loaction only <--\\n\")\n",
    "\n",
    "spark.read.option('header', True).csv(path='data/').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fded98c7-6262-437e-9da4-f05c05e2a8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|  1|  Rohit|  1000|\n",
      "|  2|   Ajay|  2000|\n",
      "|  3|Nirmala|  3000|\n",
      "|  4|    Ram|  4000|\n",
      "|  5|  Rohit|  1000|\n",
      "|  6|   Ajay|  2000|\n",
      "|  7|Nirmala|  3000|\n",
      "|  8|    Ram|  4000|\n",
      "+---+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('id', IntegerType()),\n",
    "        StructField('name', StringType()),\n",
    "        StructField('salary', IntegerType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# OR \n",
    "\n",
    "# --> Another Way to make Schema \n",
    "\n",
    "schema = StructType()\\\n",
    "        .add('id', IntegerType())\\\n",
    "        .add('name', StringType())\\\n",
    "        .add('salary', IntegerType())\n",
    "\n",
    "\n",
    "# Enforcing Schema on to dataFrame\n",
    "\n",
    "df = spark.read.option('header', True).csv(path = 'data/', schema = schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1453af53-82f0-4cb1-a5d0-ff7402a6ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1f25b-cc7d-4c93-99a7-401059d11aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
